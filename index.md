This page provides a summary list and contact links for some key computing hardware (physical and virtual) resources that are available to support research. 

## MIT cloud credits program

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| Microsoft Azure Cloud   | [https://cloud.mit.edu/credits](https://cloud.mit.edu/credits) | The cloud credits program currently has Microsoft Azure credits available. These can be used for any of the Azure Virtual machines and Azure machine learning tools. The resources include clusters of recent generation Volta GPUs with high-speed interconnects, suitable for large machine learning workflows. |
|               |                               |  |
| Google Compute Platform | [https://cloud.mit.edu/credits](https://cloud.mit.edu/credits) | Currently all credits for Google Compute Platform have been assigned to projects. |


## MIT campus wide hardware

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| Engaging    | [https://engaging-web.mit.edu](https://engaging-web.mit.edu/eofe-wiki/) | The Engaging cluster is open to everyone on campus. It has around 80,000 x86 CPU cores and 300 GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm resource scehduler that suports batch and interactive workloads and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A standard, web-based portal supporting Jupyter notebooks, R studio, Mathematica and X graphics is available at [https://engaging-ood.mit.edu](https://engaging-ood.mit.edu). Further information and support is available from [engaging-support@techsquare.com](mailto:engaging-support@techsquare.com). 
| | | |
| C3DDB | [https://c3ddb01.mit.edu/request_account](https://c3ddb01.mit.edu/request_account) | The C3DDB cluster is open to everyone on campus for research in the general area of life-sciences, health-sciences, computational biology, biochemistry and biomechanics. It has around 8000 x86 CPU cores and 100 K80 generation GPU cards. Hardware access is through the Slurm resource scheduler that suports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. Further information and support is available from [c3ddb-admin@techsquare.com](mailto:c3ddb-admin@techsquare.com).  |
| | | |
| Supercloud | [https://supercloud.mit.edu](https://supercloud.mit.edu) | The Supercloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility is open to everyone on campus. The latest Supercloud system has more than 16,000 x86 CPU cores and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource scehduler that suports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A custom, web-based portal supporting Jupyter notebooks is available at [https://txe1-portal.mit.edu/](https://txe1-portal.mit.edu/). Further inforamtion and support is available at [supercloud@mit.edu](mailto:supercloud@mit.edu).
| | | |
| Satori | [https://mit-satori.github.io](https://mit-satori.github.io) | Satori is a IBM Power 9 large memory node system. It is open to evryone on campus and has optimized software stacks for machine learning and for image stack post-processing for MIT.nano Cryo-EM faciltiies. The system has 256 NVidia Volta GPU cards attached in groups of four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the Slurm resource scehduler that suports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A standard web based portal [https://satori-portal.mit.edu](https://satori-portal.mit.edu) with Jupyter notebook support is available. Further information and support is available at [satori-support@techsquare.com](mailto:satori-support@techsquare.com).
| | | |
| submit.mit.edu | [http://submit.mit.edu](http://submit.mit.edu) | submit.mit.edu is an experimental gateway to the Open Science Grid operated by the MIT Laboratory for Nuclear Science. It is intended to be available for anyone on campus. 

## DLC shared hardware

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| TIG Shared Computing          | [https://tig.csail.mit.edu/shared-computing](https://tig.csail.mit.edu/shared-computing) | The CSAIL infrastructure group (TIG) operates an Openstack cluster and a Slurm cluster for general use by members of CSAIL. The Openstack environment supports full virtual machines. The Slurm cluster supports Singularity as a container enginer for Docker containers. Furhter information and support is available at [help@csail.mit.edu](mailto:help@csail.mit.edu).
| | |
| Openmind | [https://openmind.mit.edu](https://openmind.mit.edu)| Openmind is a shared cluster for Brain and Cognitive Science research at MIT. It has several hundred GPU cards and just under 2000 CPU cores. The cluster resources are managed by the Slurm scheduler wich provides support for batch, interactive and reservation based use. The Singulairty container system is available for executing custom Docker images. Further information and support is available from [neuro-admin@techsquare.com](mailto:neuro-admin@techsquare.com).
| | |
| LNS Computing | [http://rc.lns.mit.edu](http://rc.lns.mit.edu) | The Laboratory for Nuclear Science in Physics operates computing resources that are available to researchers within LNS.  Further information and support is available from [pra@mit.edu](mailto:pra@mit.edu).

### Kavli
### CGCS
### KochCore

## COVID-19 research support programs

## Cloud provider standard credits programs

### AWS
#### Community datasets

### Azure

### GCP

## Long term storage

### Gdrive
### Code42
### TSM
### NESE

## National facilities

 XSEDE, DOE Incite, Open Science Grid
